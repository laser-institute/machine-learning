---
title: "Considering Many Metrics"
subtitle: "Readings"
date: today 
format:
  html:
    toc: true
    toc-depth: 4
    toc-location: right
theme:
  light: simplex
  dark: cyborg
editor: visual
---

## Overview

This module was intended to instigate the interpretation of supervised machine learning models with metrics other than accuracy. These readings are intended to help you to deepen your understanding by providing a worked example and a more general argument on the use of multiple metrics to interpret supervised machine learning models. The first (Baker et al., 2020) uses *precision* (positive predictive value) as a key, relevant metric---instead of accuracy. The second (Baker et al., 2024) makes a broader argument about using an ensemble of metrics.

## Readings

**These readings are all in the `/lit` folder.**

> Baker, R. S., Berning, A. W., Gowda, S. M., Zhang, S., & Hawn, A. (2020). Predicting K-12 dropout. *Journal of Education for Students Placed at Risk (JESPAR)*, 25(1), 28-54.

> Baker, R. S., Bosch, N., Hutt, S., Zambrano, A. F., & Bowers, A. J. (2024). On fixing the right problems in predictive analytics: AUC is not the problem. *arXiv preprint*. https://arxiv.org/pdf/2404.06989

> Maestrales, S., Zhai, X., Touitou, I., Baker, Q., Schneider, B., & Krajcik, J. (2021). Using machine learning to score multi-dimensional assessments of chemistry and physics. Journal of Science Education and Technology, 30(2), 239-254.

## Reflection

To help guide your reflection on the readings, a set of guiding questions are provided below. After you have had a chance to work through one or more of the readings, **we encourage you to contribute to our learning community by creating a new post to our machine-learning channel on [Slack](http://laser-scholars.slack.com/)**. Your post might contain a response to one or more of the guiding questions, questions you still have about the topics addressed, or insights gained into your own research.

### Baker et al. (2020)

-   How do the authors leverage predictive analytics to identify students at risk of dropping out, and what are the key features used in their logistic regression model? Discuss the implications of these features on intervention strategies.

-   The paper highlights the importance of data mining techniques in educational settings. What are the potential ethical considerations and challenges associated with using predictive models to identify at-risk students?

-   Reflect on the role of socio-economic factors in the prediction of K-12 dropout rates. How can schools and policymakers address these underlying issues to improve student retention?

### Baker et al. (2024)

-   The authors argue that AUC ROC is not inherently flawed but often misused. Discuss how AUC ROC should be correctly interpreted and applied in predictive modeling, especially in educational contexts.

-   The paper critiques the reliance on single metrics for model evaluation. How can combining multiple metrics improve the robustness and fairness of predictive models? Provide examples of complementary metrics that can be used alongside AUC ROC.

-   How do the takeaways from this paper accord with or differ from the themes of this module (interpreting metrics other than accuracy)?

### Maestrales et al. (2021)

-   The study compares the accuracy of human scoring and machine learning algorithms in evaluating multi-dimensional constructed response items in science assessments. Discuss the key findings and implications of these comparisons for future educational assessments.

-   The authors highlight the challenges of achieving high agreement between human raters and machine algorithms. What strategies did the researchers employ to improve this agreement, and what were the outcomes of these strategies?

-   Reflect on the potential benefits and limitations of using machine learning to score multi-dimensional assessments. How can educators ensure the reliability and validity of these automated scoring systems in real-world classroom settings?
